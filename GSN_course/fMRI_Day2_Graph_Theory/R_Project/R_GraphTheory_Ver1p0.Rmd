---
title: "GSN Connectomics - functional MRI - Graph Theory"
output:
  html_document:
    toc: true
    theme: united
---

# load libraries

In a very first step, you need to tell R which packages you want to
load. The attached packages will be required for the analyses documented
in this script

```{r message=FALSE, warning=FALSE}
library(readxl)
library(corrplot)
library(lm.beta)
library(Hmisc)
library(qgraph)
library(plyr)
library(dplyr)
library(psych)
library(neurobase)
library(oro.nifti)
library(dplyr)
library(png)
library(ggiraphExtra)
library(data.table)
library(cowplot)
library(ggplot2)
library(patchwork)
# library(NMF)
library(ggbeeswarm)
#source("/Volumes/Users/nfranzme/R_Projects/R_Imaging/workbench/workbench_map_volume_to_surface#.R")
library(ggpubr)
library(lubridate)
library(reticulate)
library(svMisc)
library(NetworkToolbox)
library(readxl)
library(writexl)
library(lme4)
library(lmerTest)
library(stringr)
library(boot)
library(table1)

```

# load input

### define directories

set the root directory to the full path to the *fMRI_Day2_Graph_Theory*
folder

```{r}
dir.root = "/Volumes/NO NAME/fMRI_Day2_Graph_Theory/"
dir.fc.200 = paste0(dir.root, "/Schaefer200_functional_connectivity/")
dir.data.sheets = paste0(dir.root, "/data_sheets/")

```

# Data preparation

**Step 1: load and prepare the data**

The data frame contains data from the *Alzheimer's disease neuroimaging
initiative (ADNI) database*.\
ADNI is a large multi-center study that focuses on multi-modal
neuroimaging in Alzheimer's disease patients, more information can be
found here: <http://adni.loni.usc.edu>\
\
Included imaging modalities in the current dataset are **tau-PET**,
**amyloid-PET** and **resting-state fMRI**. We will subset the data to
healthy controls (i.e. cognitively normal [CN] individuals without
evidence for amyloid pathology [Ab.neg]), and subjects across the
Alzheimer's disease (AD) spectrum, defined as having abnormally elevated
amyloid-beta (Ab) based on amyloid-PET. The AD spectrum includes
individuals who are still cognitively normal (CN.Ab.pos, i.e.
preclinical AD), subjects who show mild cognitive impairment
(MCI.Ab.pos, i.e. prodromal AD), and patients with AD dementia
(Dementia.Ab.pos). An illustration of the AD cascade is shown in the
Figure below.

```{r echo=FALSE, fig.align='center'}
knitr::include_graphics(paste0(dir.root, "/images/AD_cascade.png"))

```

For this course we will work primarily with cross-sectional data, stored
in the *ADNI.data.bl* data frame. However, some individuals also have
longitudinal tau-PET, you could have a look at those data to if there is
time left.

### -\> read in data and select subjects across the AD spectrum

```{r}

# read in ADNI data
ADNI.data <- readxl::read_xlsx(paste0(dir.data.sheets, "/ADNI_fMRI_PET.xlsx"))

# we'll exclude some people who have corrupted fMRI data
ADNI.data <- subset(ADNI.data, ID %nin% c("sub-4431", "sub-2133", "sub-6559"))

# merge diagnosis and amyloid status
ADNI.data$DX.Ab <- paste0(ADNI.data$DX, ".", ADNI.data$amyloid.global.SUVR.status)
ADNI.data$DX.Ab <- factor(ADNI.data$DX.Ab, 
                          levels = c("CN.Ab.neg", 
                                     "CN.Ab.pos", 
                                     "MCI.Ab.pos", 
                                     "Dementia.Ab.pos"))

# subset to controls and subjects across the AD spectrum 
# (CN = cognitively normal, 
# MCI = Mild Cognitive Impairment, 
# Dementia = Alzheimer's disease dementia)

ADNI.data <- subset(ADNI.data, 
                    DX.Ab %in% c("CN.Ab.neg", 
                                 "CN.Ab.pos", 
                                 "MCI.Ab.pos", 
                                 "Dementia.Ab.pos"))

# locate functional connectivity matrices
ADNI.data$FC.mat.200 <- str_replace(ADNI.data$FC.mat.200, 
                                    "/Network/Cluster/ADNI/functional_connectivity/Schaefer_200/", 
                                    paste0(dir.root, "/Schaefer200_functional_connectivity/"))

# select baseline data
ADNI.data.bl = subset(ADNI.data, tau.pet.fu.yrs == 0)


```

### -\> create a first overview table

First, you should get an idea of the type of data we're working with. To
this end we will check some basic distributions of biomarker and
cognitive data for the current patient cohort.

#### create "table 1"

We can first create an overview table of demographics, as well as
amyloid-PET and tau-PET load, to\
see how biomarkers are distributed across diagnostic groups.\
We'll look at the following variables (Variable names on the left,
explanation on the right)

*age = age\
sex = sex\
centiloid = global amyloid-PET level (a typical "cut-off" for
abnormality is 20)\
tau.global.SUVR = global tau-PET level (a typical "cut-off" for
abnormality is 1.3)\
MMSE = Mini Mental State Exam, i.e. a global cognitive screening
instrument, where lower values indicate stronger cognitive impairment\
ADNI_MEM = A memory composite score across several memory tests. Lower
values indicate stronger impairment*

```{r}

table1(~ age + factor(sex) + centiloid + tau.global.SUVR + MMSE + ADNI_MEM | DX.Ab, data = ADNI.data.bl)
```

As you can see, amyloid (*centiloid*) and tau levels
(*tau.global.SUVR*)\
increase (i.e. become more abnormal) across more severe disease levels.\
Also, *MMSE* and *ADNI_MEM* values decrease, indicating stronger
cognitive impairment.

#### plot biomarker distributions

Next, we should visualize distributions of amyloid and tau biomarkers,\
to get an idea of how amyloid and tau relate to increasing disease
severity.\
We'll use ggplot2 for plotting. Some useful ggplot2 tutorials can be
found here:

<https://ggplot2.tidyverse.org>\
<http://r-statistics.co/Complete-Ggplot2-Tutorial-Part1-With-R-Code.html>\
<http://r-statistics.co/ggplot2-Tutorial-With-R.html>

##### 1. Amyloid-PET

Let's first create a boxplot/beeswarm plot to illustrate the amyloid
distribution across different diagnostic groups. The variables we'll
look at are *centiloid* (i.e. amyloid-PET) and *DX.Ab* (i.e. diagnostic
groups)

```{r, fig.align='center'}

# plot group differences
ggplot(data = ADNI.data.bl,
       aes( 
         x = DX.Ab,
         y = centiloid)) + 
  geom_beeswarm() + 
  geom_boxplot(alpha = 0.1, notch = T, width = 0.5) + 
  theme_minimal() + 
  geom_hline(yintercept = 20, linetype = 2)


```

As you can see, amyloid levels increase across the AD spectrum. The
**dashed line** indicates where amyloid becomes abnormal.

To statistically test group differences in amyloid, we can run an
ANCOVA. It's quite common standard to control for some covariates in
clinical research, such as *age* and *sex* (and often also *education*)

So, lets quantify the group differences using an ANCOVA with a post-hoc
Tukey Test. The ANCOVA tests whether there is an overall
group-difference between diagnostic groups. The Post-Hoc Tukey test
assesses the difference between single diagnostic groups. ANCOVAs can be
run using the *aov* command and requires a data frame as input (here it
is *ADNI.data.bl*), as well as an equation that describes the
relationship between dependent and independent variables.

Here, the equation is *centiloid \~ DX.Ab + age + sex,* which means that
we test group differences in *centiloid* between *DX.Ab* groups, while
controlling for *age* and *sex.*

```{r warning=FALSE}
# test group differences (ANCOVA), controlling for age and sex

## run ANCOVA
tmp.aov <- aov(data = ADNI.data.bl,
               centiloid ~ DX.Ab + age + sex); 

## summarize output
summary(tmp.aov)

## run post-hoc Tukey test to determine group differences
TukeyHSD(tmp.aov, which = "DX.Ab")


```

##### 2. Tau-PET

Let's do the same for tau-PET, so lets first create a boxplot/beeswarm
plot to illustrate the tau distribution across diagnostic groups

```{r, fig.align='center'}

# plot group differences
ggplot(data = ADNI.data.bl,
       aes( 
         x = DX.Ab,
         y = tau.global.SUVR)) + 
  geom_boxplot(alpha = 0.1, notch = T, width = 0.5) + 
  geom_beeswarm() + 
  theme_minimal() + 
  geom_hline(yintercept = 1.3, linetype = 2)


```

As you can see, tau levels also increase across the AD spectrum.
However, people usually start surpassing the threshold at later stages
than for amyloid. This is the case because amyloid precedes symptom
onset by decades, while tau pathology is much closer to symptom onset.
In fact, tau pathology is the key driver of neurodegeneration and
cognitive decline, so you typically see abnormal tau first when people
start to show symptoms.

To statistically test these group differences, we can again run an
ANCOVA with a post-hoc Tukey Test

```{r warning=FALSE}
# test group differences (ANCOVA), controlling for age and sex

## run ANCOVA
tmp.aov <- aov(data = ADNI.data.bl,
               tau.global.SUVR ~ DX.Ab + age + sex); 

## summarize output
summary(tmp.aov)

## run post-hoc Tukey test to determine group differences
TukeyHSD(tmp.aov, which = "DX.Ab")


```

##### 3. Staging of tau pathology

One thing to keep in mind is that the above described analyses on
tau-PET are based on "global" tau levels. However, tau does not - in
contrast to amyloid - accumulate globally throughout the brain, but
rather follows a consecutive spreading pattern that typically starts in
the inferior temporal lobe. This "spreading pattern" of tau pathology
has been already described in the nineties by Braak & Braak.

```{r echo=FALSE}
knitr::include_graphics(paste0(dir.root, "/images/Braak_stage_Tau_positivity_sequence_Braak2_excl-01.jpg"))

```

The surface rendering shows the six Braak-stages. Abnormal tau pathology
is typically found first in Stage 1 (Entorhinal cortex) and then spreads
to the hippocampus (Stage 2), the inferior temporal and limbic cortex
(Stages 3&4) and finally to the association and primary somatosensory
cortex (Stages 5&6). The plot on the left illustrates the likelihood of
tau abnormality within each Braak stage across a group of AD patients,
showing that early Braak stages typically show earlier tau abnormality
than later Braak stages. Note that we usually discard the hippocampus
when looking at tau-PET, because the tau-PET tracer shows quite
significant off-target binding in the choroid plexus which is right next
to the hippocampus. Thus, tau-PET signal in the hippocampus is quite
confounded, at least for first-generation tau-PET tracers like
flortaucipir.

Next, lets look at the distribution of tau pathology in the separate
Braak stage ROIs. Tau-PET within these Braak-stage ROIs is labeled as
*tau.braak1.SUVR, tau.braak3.SUVR, tau.braak4.SUVR*, etc.

```{r, fig.width=4, fig.height=13, fig.align='center'}

# plot group differences
# note that you can store plots in the workspace (e.g. p1 below) and later combine them using the patchwork package
p1 <- ggplot(data = ADNI.data.bl,
       aes( 
         x = DX.Ab,
         y = tau.braak1.SUVR)) + 
  geom_boxplot(alpha = 0.1, notch = T, width = 0.5) + 
  geom_beeswarm() + 
  theme_minimal() + 
  geom_hline(yintercept = 1.3, linetype = 2) + 
  ggtitle("Braak 1")

p2 <- ggplot(data = ADNI.data.bl,
       aes( 
         x = DX.Ab,
         y = tau.braak3.SUVR)) + 
  geom_boxplot(alpha = 0.1, notch = T, width = 0.5) + 
  geom_beeswarm() + 
  theme_minimal() + 
  geom_hline(yintercept = 1.3, linetype = 2) +
  ggtitle("Braak 3")


p3 <- ggplot(data = ADNI.data.bl,
       aes( 
         x = DX.Ab,
         y = tau.braak4.SUVR)) + 
  geom_boxplot(alpha = 0.1, notch = T, width = 0.5) + 
  geom_beeswarm() + 
  theme_minimal() + 
  geom_hline(yintercept = 1.3, linetype = 2) +  
  ggtitle("Braak 4")


p4 <- ggplot(data = ADNI.data.bl,
       aes( 
         x = DX.Ab,
         y = tau.braak5.SUVR)) + 
  geom_boxplot(alpha = 0.1, notch = T, width = 0.5) + 
  geom_beeswarm() + 
  theme_minimal() + 
  geom_hline(yintercept = 1.3, linetype = 2) + 
  ggtitle("Braak 5")


p5 <- ggplot(data = ADNI.data.bl,
       aes( 
         x = DX.Ab,
         y = tau.braak6.SUVR)) + 
  geom_boxplot(alpha = 0.1, notch = T, width = 0.5) + 
  geom_beeswarm() + 
  theme_minimal() + 
  geom_hline(yintercept = 1.3, linetype = 2) + 
  ggtitle("Braak 6")

# you can combine different plots using the patchwork package
# a "/" will add a new plot below, a "+" will add a plot next to the first one
p1 / p2 / p3 / p4 / p5

```

You can see that earlier Braak stage ROIs show abnormal signal more
early during the disease course than later Braak stage ROIs

##### 4. Association between amyloid and tau

Our current understanding of Alzheimer's disease suggests that
amyloid-beta deposition is the first pathological event to happen, that
triggers downstream tau accumulation, neurodegeneration and cognitive
decline. Thus, tau and amyloid should be correlated. Let's have a look:

```{r, fig.align='center'}

ggplot(ADNI.data.bl,
       aes(
         x = centiloid,
         y = tau.global.SUVR
       )) + 
  geom_point() + 
  geom_smooth(method = "lm") + 
  theme_minimal()



```

This shows that higher amyloid is clearly associated with higher tau
pathology. We can quantify the association using correlation or linear
regression controlling for age and sex.

To assess the correlation, we use the *rcorr* command from the *Hmisc*
package.\
The input arguments are just two numeric vectors of the same length.

```{r}
# correlation
rcorr(ADNI.data.bl$centiloid,
      ADNI.data.bl$tau.global.SUVR)

```

To run a linear regression, we use the *lm* command. This works very
similar to the *aov* command shown above for determining group
differenecs. All we need is a data frame that contains our data
*(ADNI.data.bl),* as well as an equation that describes the relationship
between our independent variable *tau.global.SUVR* and our dependent
variable *centiloid,* while controlling for age and sex.

```{r}
# linear regression controlling for age and sex
tmp.lm <- lm(data = ADNI.data.bl,
                     tau.global.SUVR ~ centiloid + age + sex); summary(tmp.lm)
```

## TASK 1

***Now that you know how to run group comparisons (i.e. ANCOVAs) and
correlations/regression (linear models), try to check whether amyloid
and tau are associated with cognition (e.g. MMSE and ADNI_MEM). How do
amyloid and tau relate to cognitive performance? Try to create figures
and statistics showing differences in cognitive performance between
diagnostic groups, and associations between amyloid, tau and
cognition.***

To this end, you can create a new R-script, just click on the top left
*File \> New File \> R-script* as shown below. A new script will open
up, where you can generate and run code.

```{r echo=FALSE, fig.align='center'}
knitr::include_graphics(paste0(dir.root, "/images/Create_script.png"))

```

# fMRI analysis

Not that we're familiar with the basics of AD biomarkers and R, lets go
have a look at fMRI parameters (since this is actually a course on
connectomics). I have prepared functional connectivity data using a 200
ROI cortical atlas (see <https://pubmed.ncbi.nlm.nih.gov/28981612/>),
that we commonly use for fMRI analyses. A surface rendering of the atlas
can be found below, including the seven networks (DMN = Default-Mode,
DAN = Dorsal Attention, VAN = Ventral Attention, FPCN = Fronto-parietal
control, Visual, Motor, Limbic). If you want to get some methodological
details on how we processed the data, please refer to our previous paper
here: <https://www.nature.com/articles/s41467-019-14159-1>.

To compute functional connectivity, we have extracted BOLD timeseries
for each ROI, which were then correlated between ROIs.

```{r echo=FALSE, fig.align='center'}
knitr::include_graphics(paste0(dir.root, "/images/Schaefer200.png"))

```

## 1. Importing connectivity matrices

Atlas based connectivity is usually stored in matrix format also
referred to as an ***adjacency matrix***. For 200 ROIs this means that
the resulting adjacency matrix is 200x200 elements large, where each
element corresponds to the connectivity-metric between two ROIs (e.g.
***Fisher-z transformed Pearson correlation***). All connectivity
matrices are stored in the *"Schaefer200_functional_connectivity"*
folder on the hard drive in *.csv* format. The connectivity matrix are
200 x 200 where each row and column corresponds to an ROI (see below).
The diagnonal contains autocorrelations and is thus set to infinite
values.

```{r echo=FALSE, fig.align='center'}
knitr::include_graphics(paste0(dir.root, "/images/Connectivity_matrix.png"))

```

This path to the connectivity folder is already stored in your R
worksapce as *dir.fc.200.* The paths to the matrices of each participant
are stored in the *ADNI.data.bl* data frame in the variable
*FC.mat.200.* You can access this variable by navigating with the *\$*
sign, e.g. *ADNI.data.bl\$FC.mat.200*. Lets load an example connectivity
matrix from one of the ADNI participants to see how this looks like.

```{r}
# import and format connectivity matrix using the read.csv command
current.adj.matrix = read.csv(
  paste0(dir.root, 
         "Schaefer200_functional_connectivity/Schaefer_200-ADNIsub-4187_ses-101_task-rest_bold_mcf_covremoved_detrended_filtered_MNI_8mm_smoothed.csv"))
  

```

When checking the dimension of this matrix, we realize that there are in
fact 201 columns and 200 rows. The first column contains the rownames,
so we need to remove it in order to get a symmetrical 200x200 matrix.

```{r}
# check dimensions
dim(current.adj.matrix)

# remove first column
current.adj.matrix.cleaned <- current.adj.matrix[,-1]

# clean diagnonal (i.e. remove infinite values with zeros)
diag(current.adj.matrix.cleaned) = 0

# transform to matrix format
current.adj.matrix.cleaned.matrix <- as.matrix(current.adj.matrix.cleaned)

# check dimensions
dim(current.adj.matrix.cleaned)


```

Now the connectivity matrix has 200 rows and 200 columns. Lets have a
look at the overall connectivity pattern. You can plot connectivity
matrices using the *"corrplot"* package.

```{r, fig.align='center'}
# plot correlation matrix
corrplot::corrplot(current.adj.matrix.cleaned.matrix, 
                   diag = FALSE, 
                   tl.pos = "n", 
                   tl.cex = 0.5, 
                   method = "color", 
                   is.corr = FALSE)

```

You can see that the connectivity matrix is symmetrical, with positive
correlations and negative correlations. Positive correlations (blue
colors) mean that two brain regions show correlated BOLD signal,
suggesting that they're functionally connected. Negative correlations
indicate anti-correlations, where one region is up-regulated and another
one is down-regulated at the same time.

## 2. Preparing connectivity matrices for graph theoretical analyses

In order to perform graph theoretical analyses, we usually perform some
additional preprocessing steps, e.g. we omit negative connections or
perform thresholding of the matrix.

I've attached a function that can perform some basic thresholding
operations for you. The function below can be called from the command
line, and needs several input arguments:

*adj_matrix* = a symmetrical adjacency matrix,\
*retain_negatives* = a boolean statement (TRUE/FALSE) indicating whether
or not negative values should be retained,\
*density* = a percentage of connections that should be retained. A
density of 1 indicates that 100% of connections should be retained, a
density of 0.3 means that only 30% of the strongest connections will be
retained,\
*replace_with_NAs* = a boolean statement (TRUE/FALSE) indicating whether
values that are eliminated from the matrix should be replaced with an NA
(not assigned). Otherwise, values are replaced with a zero.

If you're interested in how to write functions in R, you can refer to
one of the following tutorials:
<https://swcarpentry.github.io/r-novice-inflammation/02-func-R/>,
<https://r4ds.had.co.nz/functions.html>

```{r}
# define matrix thresholding function

adj_mat_density_thresh=function(adj_matrix, retain_negatives, density, replace_with_NAs){
  # exclude negative values
  if (retain_negatives==F){adj_matrix[adj_matrix<0]=0}
  # determine threshold
  threshold=quantile(abs(adj_matrix), 1-density)
  # apply threshold
  if (replace_with_NAs==F){adj_matrix[abs(adj_matrix)<threshold]=0}
  if (replace_with_NAs==T){adj_matrix[abs(adj_matrix)<threshold]=NA}
  
  return(adj_matrix)
  
}

```

Next, lets threshold the matrix that we've imported above and eliminate
all the negative values (i.e. *retain_negatives = F*), while keeping all
positive values (i.e. *density = 1*). Everything else should be replaced
with a zero.

#### 2.1. Removing negative connections

```{r}
# call the function and store the output in a new variable
current.adj.matrix.cleaned.matrix.thr1 <- adj_mat_density_thresh(adj_matrix = current.adj.matrix.cleaned.matrix, 
                                                                retain_negatives = F, 
                                                                density = 1, 
                                                                replace_with_NAs = F)

```

Let's see how the new matrix looks like:

```{r, fig.align='center'}
# plot correlation matrix
corrplot::corrplot(current.adj.matrix.cleaned.matrix.thr1, 
                   diag = FALSE, 
                   tl.pos = "n", 
                   tl.cex = 0.5, 
                   method = "color", 
                   is.corr = FALSE)
```

You can see that all the negative red values have been eliminated, while
the positive values have been kept.

#### 2.2. Density thresholding

Next, lets see what happens if we further threshold the matrix. As an
example, we want to retain only 10% of the strongest positive
connections (i.e. a density of 0.1). Thresholding is often performed to
remove "noisy" connections.

```{r}
# call the function and store the output in a new variable
current.adj.matrix.cleaned.matrix.thr0p1 <- adj_mat_density_thresh(adj_matrix = current.adj.matrix.cleaned.matrix, 
                                                                retain_negatives = F, 
                                                                density = 0.1, 
                                                                replace_with_NAs = F)

```

Let's see how the thresholded matrix looks like:

```{r, fig.align='center'}
# plot correlation matrix
corrplot::corrplot(current.adj.matrix.cleaned.matrix.thr0p1, 
                   diag = FALSE, 
                   tl.pos = "n", 
                   tl.cex = 0.5, 
                   method = "color", 
                   is.corr = FALSE)
```

You can see that the matrix looks quite sparse now and only the
"backbone" of very strong connections is retained. Sometimes
thresholding is done to eliminate "noisy" and "weak" connections.
However, these weak connections have actually been shown to not only
consist of noise, but to encode some important inter-individual
differences, as shown by papers on "connectome fingerprinting" (see work
by Emily Finn <https://www.nature.com/articles/nn.4135>).

#### 2.3. Binarization

While the matrix above is relatively sparse, it still includes
information about the "strength" of a connection, which we refer to as a
*weighted matrix* Some researchers, however, use *binary* matrices,
which just encodes the presence or absence of a connection. This is
often done for structural connectivity matrices. Binarization can be
done quite easily using the *NetworkToolbox*. So lets binarize the
matrix in which we've retained only 10% of the strongest connections

```{r, fig.align='center'}
current.adj.matrix.cleaned.matrix.thr0p1.bin <- 
  NetworkToolbox::binarize(current.adj.matrix.cleaned.matrix.thr0p1)

# plot correlation matrix
corrplot::corrplot(current.adj.matrix.cleaned.matrix.thr0p1.bin, 
                   diag = FALSE, 
                   tl.pos = 'n', 
                   tl.cex = 0.5, 
                   method = "color", 
                   is.corr = FALSE)
```

Now, all information about the strength of connections is lost, and
we've retained the 10% of strongest connections. Next, we will look at
some graph theoretical parameters (which can be computed on all sorts of
graphs) to study complex network function. Keep in mind that the the way
you process the graphs can drastically change its' appearance. So
thresholding and binarizing your graph should always be motivated by
theoretical considerations.

## 3. Graph theory

So lets use the connectivity matrix to compute some basic graph
theoretical parameters. Graph theory is a mathematical approach to study
networks that are defined by nodes that are connected via edges (see
below). In our case, nodes are Regions of interest (ROIs) of the 200-ROI
brain atlas, and the edges are the connectivity strength between the
different nodes/ROIs. There are different types of graphs, including
binary graphs, weighted graphs, and directed graphs that also encode
information about the directionality of information flow. In fMRI, we
usually work with weighted undirected graphs.

```{r echo=FALSE, fig.align='center', out.width="80%"}
knitr::include_graphics(paste0(dir.root, "/images/Graphs.png"))

```

In fact, we can already render our connectivity matrix as a graph using
the qplot command qgraph. To this end, we also read in the network
affiliation or community vector, which tells us to which network a given
ROI belongs to (e.g. DMN, DAN, Limbic, Motor etc.)

```{r, fig.align='center'}
# load community vector
Community <- read.table(
  paste0(dir.root, 
         "Atlas/Schaefer2018_200Parcels_7Networks_order_numeric.txt"))

Community$names <- mapvalues(Community$V1, 
                             from=c(1,2,3,4,5,6,7), 
                             to=c("Visual", 
                                  "Motor", 
                                  "DAN", 
                                  "VAN", 
                                  "Limbic", 
                                  "FPCN", 
                                  "DMN"))
# create plot
qgraph(current.adj.matrix.cleaned.matrix.thr0p1, 
       groups = Community$names, 
       layout = "spring", 
       labels=F, 
       color=c("forestgreen" ,
               "indianred2" ,
               "darkgoldenrod1" ,
               "lemonchiffon1" ,
               "skyblue3" ,
               "mediumorchid1" ,
               "magenta4"), 
       posCol=c("gray68", "gray49"))
```

## TASK 2

***Now that you know how to render graphs, you can check how applying
different thresholds changes the appearence of your graph.***

[***So why do we need graph theory at all?***]{.ul}\
The advantage of graph theory is that it allows to derive more complex
parameters that describe network topology, such as measures of
modularity, segregation and integration. A great overview of different
graph theoretical measures can be found in the landmark paper by Rubinov
& Sporns from 2010 (<https://pubmed.ncbi.nlm.nih.gov/19819337/>). The
paper is, however, not open access, so I've stored a copy in the
*/fMRI_Day2_Graph_Theory/Literature/* folder for you.

[***How do we compute graph theoretical parameters?\
***]{.ul}For computing graph theoretical parameters, we can use various
different toolboxes and programs. The most famous one is the *brain
connectivity toolbox* in matlab, which was developed by Olaf Sporns and
colleagues (see <https://sites.google.com/site/bctnet/>). However,
matlab is quite old-fashioned and not open source, so people (including
me) have moved more and more away from using it. There are many newer
toolboxes for python (e.g. <https://pypi.org/project/bctpy/>) and R,
which can basically do the same things. Since we're working in R, we
will use the *"NetworkToolbox"*,which comes with some easy to use
commands (see
[[\<https://cran.r-project.org/web/packages/NetworkToolbox/NetworkToolbox.pdf\>](https://cran.r-project.org/web/packages/NetworkToolbox/NetworkToolbox.pdf).￼Alternatively))](https://cran.r-project.org/web/packages/NetworkToolbox/NetworkToolbox.pdf){.uri}\
Or alternatively, you can code the functions yourself. This sounds
complicated, but really isn't once you get a bit more skilled in coding.

[***Which parameters do we cover?\
***]{.ul}We'll have a look at some basic parameters of network
segregation and network integration. Specifically, we'll look at
clustering, degree, within-network connectivity, between-network
connectivity (community strength), path-length, small-worldness and
network segregation.

### 3.1. Clustering

Let's start with clustering. The clustering coefficient quantifies the
abundance of connected triangles in a network and is a major descriptive
statistics of networks. This means, if a node is connected to two
neighbours, and if these neighbours are also interconnected, they form a
cluster (triangle), as illustrated in the example below. The clustering
coefficient just counts the number of triangles that can be found in a
given community of nodes. Clustering is most often referred to as a
measure of segregation, indicating how strong local communities are
interconnected (e.g. whether your friends are also friends among each
other)

```{r echo=FALSE, fig.align='center'}
knitr::include_graphics(paste0(dir.root, "/images/Clustering.png"))

```

So lets compute the clustering coefficient on our connectivity matrix.
We can compute a "Global" clustering coefficient for the entire network,
as well as a clustering coefficient for each single node in the network.
In fact, the global clustering coefficient is just the average
clustering across all network nodes (so 200 in our case). To compute the
clustering coefficient, we use the *clustcoeff* command from the
*NetworkToolbox*. This command requires two inputs, an adjacency matrix
and a TRUE/FALSE statement on whether the matrix is weighted or binary.
You can find information about this function by calling
*help("clustcoeff")* from the console. Clustering is usually computed on
networks with "positive" connections only, hence we need to use a
connectivity matrix from which we have eliminated the negative
connections called *current.adj.matrix.cleaned.matrix.thr1* (see chapter
*2.1. Removing negative connections*). We can compute the clustering
coefficient for networks thresholded at a density of 1 and 0.1 to see
how the clustering coefficient changes across thresholds.

```{r}
# compute clustering 
tmp.clustering.weighted.thr1 <- clustcoeff(current.adj.matrix.cleaned.matrix.thr1, weighted = T)
tmp.clustering.weighted.thr0p1 <- clustcoeff(current.adj.matrix.cleaned.matrix.thr0p1, weighted = T)

```

The output contains two different measures, the global clustering
coefficient *CC*, and the local clustering coefficient for each ROI
called *CCi*. You can choose which one to look at by navigating with the
*\$* sign.

```{r}
# global clustering
tmp.clustering.weighted.thr1$CC
tmp.clustering.weighted.thr0p1$CC

# ROI-wise clustering at a density threshold of 1
tmp.clustering.weighted.thr1$CCi
# distribution of ROI-wise clustering at a density threshold of 1
hist(tmp.clustering.weighted.thr1$CCi)

# plot the association of clustering coefficients across different thresholds
plot(tmp.clustering.weighted.thr1$CCi, tmp.clustering.weighted.thr0p1$CCi)

# quantify the association of clustering coefficients across different thresholds
rcorr(tmp.clustering.weighted.thr1$CCi, tmp.clustering.weighted.thr0p1$CCi)

```

You can see that clustering is much higher in the network that was
thresholded more restrictively. Any idea/hypothesis why this could be
the case?

### 3.2. Degree

The degree of a node quantifies the number or strength of connections
that a given node has to the rest of the network (see figure below)

```{r echo=FALSE, fig.align='center', out.width="50%"}
knitr::include_graphics(paste0(dir.root, "/images/Degree.png"))

```

The degree can also be computed relatively simple as the row or column
Means of a adjacency matrix, which just quantifies the strength of
connections that a given node has to the remaining 199 nodes.

```{r}

# compute degree
tmp.degree.weighted.thr1 <- colMeans(current.adj.matrix.cleaned.matrix.thr1, na.rm = T)
hist(tmp.degree.weighted.thr1)
tmp.degree.weighted.thr0p1 <- colMeans(current.adj.matrix.cleaned.matrix.thr0p1, na.rm = T)
hist(tmp.degree.weighted.thr0p1)

# correlation of clustering coefficients across different thresholds
plot(tmp.degree.weighted.thr1,tmp.degree.weighted.thr0p1)
rcorr(tmp.degree.weighted.thr1,tmp.degree.weighted.thr0p1)
```

### 3.3. Within-network connectivity

One of the most simple measures that we can determine in connectomics is
a parameter of within-network connectivity. For example, we can
calculate the mean connectivity among nodes of the default mode network.
To this end, we use the *participation* command of the network toolbox.
Again, this command requires a matrix ***A*** as an input, as well as a
***community vector*** that assigns the different nodes to networks.
Thus, we need to know first to which network a given node belongs to. To
this end, we import the community vector for the Schaefer200 atlas.

```{r}
# load community vector
Community <- read.table(paste0(dir.root, "Atlas/Schaefer2018_200Parcels_7Networks_order_numeric.txt"))
Community$names <- mapvalues(Community$V1, 
                             from=c(1,2,3,4,5,6,7), 
                             to=c("Visual", "Motor", "DAN", "VAN", "Limbic", "FPCN", "DMN"))

# check the number of ROIs assigned to each node
table(Community$names)
```

Next, we compute the within network connectivity using the
*participation* command

```{r}
tmp.particpation.weighted.thr1 <- 
  participation(A = current.adj.matrix.cleaned.matrix.thr1, 
                comm = Community$names)

# now we get a measure of how strong a given node is connected to other nodes of the same network
print(tmp.particpation.weighted.thr1)



# To summarize this for each network, we just create the average 
tmp.df <- data.frame(network.connectivity = tmp.particpation.weighted.thr1$positive, 
                     network = Community$names)

boxplot(tmp.df$network.connectivity~tmp.df$network)


tmp.df.mean <- 
  tmp.df %>% 
  group_by(network) %>% 
  summarise(network.connectivity = mean(network.connectivity))

print(tmp.df.mean)


```

### 3.4. Between-network connectivity

We can also compute the connection strength of a given network (i.e.
nodes surrounded by a gray circle), to the remaining networks. An
example of this would be, how strong the DMN is connected to the rest of
the brain. This is again a measure of integration, as it quantifies the
"cross-talk" between networks.

```{r echo=FALSE, fig.align='center'}
knitr::include_graphics(paste0(dir.root, "/images/Community.jpg"))

```

Next, lets compute the between network connectivity, using the
*comm.str* command. We need three input arguments, A = the connectivity
matrix comm = the community vector (i.e. which network does a node
belong to) weighted = a boolean (TRUE/FALSE), stating whether the
network is weighted or not

```{r, out.width="40%"}
tmp.between.network.connectivity = comm.str(A = current.adj.matrix.cleaned.matrix.thr1,
         comm = Community$names,
         weighted = T,
         measure = c("between"))
print(tmp.between.network.connectivity)

```

The output reflects the strength of connections from a given network to
the remaining networks

### 3.5. Path-length

Next, we want to determine a measure of network integration, called
shortest path length. This measure quantifies, how many steps (or
"connections") a given node is away from any other node in the network
on average. The measure was originally developed in social psychology,
showing that a given person may know any other person in the world via
just seven other people (or "connections"). This phenomenon was thus
termed "small-world", since networks usually contain short cuts that
cross-link distant nodes (like highways). Accordingly, a network that is
segregated in modules, but shows shortcuts between the modules is called
a "small-world" network.

```{r echo=FALSE, fig.align='center', out.width="80%"}
knitr::include_graphics(paste0(dir.root, "/images/Pathlength.jpg"))

```

So what we will do is to determine the path length of each node, that is
how "far" in terms of connectedness a given node is away from all the
remaining nodes in the network. Again, the computation is super simple
using the *pathlengths* command of the *NetworkToolbox*, but I recommend
some reading if you're interested understand the maths behind it (e.g.
the paper by Rubinov & Sporns
<https://pubmed.ncbi.nlm.nih.gov/19819337/>). As always, if you want
more information on the *pathlengths* command, run *help("pathlengths")*
from the console. The function requires two inputs, *A* = the adjacency
matrix and *weighted* = a TRUE/FALSE statement indicating whether the
matrix is weighted or not.

```{r}
# compute path lengths
tmp.pathlengths <- pathlengths(current.adj.matrix.cleaned.matrix.thr1,
                               weighted = TRUE)

```

The output contains several measures, including a global measure of
*average shortest path length* called *ASPL*, which is the mean path
length of the entire network. A network with on average short path
length tends to be relatively integrated with fast communication among
its' nodes. In addition, we get a path length measure for each node,
called *ASPLi*

```{r}
# average shortest path length of the entire network
tmp.pathlengths$ASPL

# average shortest path length of each node
tmp.pathlengths$ASPLi

hist(tmp.pathlengths$ASPLi)
```

### 3.6. Small-worldness

Speaking of small-world networks, we can also directly quantify how much
a given network architecture resembles a small world network. Here's a
short definition of small-world networks:

*A small-world network is a type of mathematical graph in which most
nodes are not neighbors of one another, but the neighbors of any given
node are likely to be neighbors of each other and most nodes can be
reached from every other node by a small number of hops or steps.*

In other words, small-world networks tend to be an intermediate
phenotype between a regular network and a random network (see figure
below). In biology, networks tend to orient towards small-world topology
(see for instance
<https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3604768/>), which seems to
be a particularly cost-efficient topology.

```{r echo=FALSE, fig.align='center', out.width="80%"}
knitr::include_graphics(paste0(dir.root, "/images/Smallworld.png"))

```

To compute the "small-worldness" of a given network, we can use the
*smallworldness* command of the *NetworkToolbox*. This requires several
inputs. *A* = an adjacency matrix, *iter* = the number of iteratively
generated random networks with equivalent weight and degree distribution
that are generated against which "your" network is compared against.
Note that computing this parameter takes some time, and the computation
time gets longer the more iterations you select. For help, call
*help("smallworldness")* from the console.

```{r}
# set a fixed seed to get reproducible results when generating random data
set.seed(42)

# compute small-worldness coefficient
tmp.smallworldness <- smallworldness(current.adj.matrix.cleaned.matrix.thr1,
                                     iter = 10,
                                     progBar = FALSE,
                                     method = "HG")

# show small-world coefficient
print(tmp.smallworldness$swm)

```

### 3.7. Comparing different measures of network integration and segregation

It's also quite useful to compare different graph theoretical metrics,
because many of them are highly inter-related. When we compare the
average shortest path length (x-axis) as a measure of integration with
another measure of segregation (e.g. clustering, y-axis), we can see
that nodes with short path length also tend to show higher clustering.
This is because a node with many connections to his neighbors can also
reach the remaining network faster via less connections.

```{r}
plot(tmp.pathlengths$ASPLi, tmp.clustering.weighted.thr1$CCi)
rcorr(tmp.pathlengths$ASPLi, tmp.clustering.weighted.thr1$CCi)

```

You can also have a look at how these metrics distribute across the
different networks

```{r}
# path length
boxplot(tmp.pathlengths$ASPLi~Community$names)

# clustering
boxplot(tmp.clustering.weighted.thr1$CCi~Community$names)

# degree
boxplot(tmp.degree.weighted.thr1~Community$names)

```

## 4. Computing graph theoretical parameters for groups of subjects

Now that you've learned about some basic graph theoretical parameters
(there are many more!), it's time to learn how to compute these measures
for larger groups of subjects. Running the code above "by hand" for
every individual from the current dataset would possible take ages
(...and it would be super boring...). So lets do what I call "applied
laziness" and run everything automatically using looped scripts and
functions. The basic idea is that we define a set of fixed analysis
steps and repeat them iteratively across many individuals. The simplest
way to do this is to use *for loops* (see for instance
<https://www.datacamp.com/community/tutorials/tutorial-on-loops-in-r>).
And while the computer is getting the work done for you, you can go and
do whatever you want...while still getting paid for your job... ;)

### 4.1. running for loops

So we need to define a basic analysis scheme that we would like to apply
to our data. For a given subject, we first need to read in the
connectivity matrix, apply some thresholding and preprocessing, and then
compute measures of clustering, degree, community strength, path-length
and small-worldness. Basically we'll repeat all the steps above in a
single script. To keep it rather simple, we compute the clustering
coefficient, degree, and path-length measures for each node and then
summarize these measures for the whole brain and for the seven different
networks. Note that we will overwrite some of the file names in the code
above.

```{r eval=FALSE, warning=FALSE, include=FALSE}
# define your input data
input.data = ADNI.data.bl

# define the number of iterations that need to be performed. 
# In our data frame, each row corresponds to one subject, so we want to iterate across rows.
# Thus, the number of iterations corresponds to the number of rows of our input data

n.iter = nrow(input.data)
print(paste0("number of iterations = ", n.iter))

# Next, open the loop, and define that it runs all 
# the way from 1 to the number of iterations that you've defined as n.iter
# within the loop, you will use the letter i which will change in each iteration from 1, to 2, to 3 to n.iter

n = 0
for (i in 1:n.iter){
  svMisc::progress(i, max.value = n.iter)
  # set an internal counter within the loop (can be helpful sometimes!)
  n = n +1
  
  # select the i-th observation in the data frame
  current.data = input.data[i,]
  
  # import the connectivity matrix
  current.adj.matrix = read.csv(current.data$FC.mat.200[1])
  
  # remove first column
  current.adj.matrix.cleaned <- current.adj.matrix[,-1]

  # clean diagnonal (i.e. remove infinite values with zeros)
  diag(current.adj.matrix.cleaned) = 0

  # transform to matrix format
  current.adj.matrix.cleaned.matrix <- as.matrix(current.adj.matrix.cleaned)

  # remove negative connections and perform density thresholding at a density of 1
  current.adj.matrix.cleaned.matrix.thr1 <- adj_mat_density_thresh(adj_matrix = current.adj.matrix.cleaned.matrix, 
                                                                retain_negatives = F, 
                                                                density = 1, 
                                                                replace_with_NAs = F)
  
  # compute clustering coefficient
  current.clustering.weighted.thr1 <- clustcoeff(current.adj.matrix.cleaned.matrix.thr1, weighted = T)
  
  # compute degree
  current.degree.weighted.thr1 <- colMeans(current.adj.matrix.cleaned.matrix.thr1, na.rm = T)

  # compute community strength
  current.comm.str.within = comm.str(A = current.adj.matrix.cleaned.matrix.thr1,
         comm = Community$names,
         weighted = T,
         measure = c("within"))
  
  # compute participation coefficient
  current.participation = participation(current.adj.matrix.cleaned.matrix.thr1, comm = Community$names)
  
  # compute path lengths
  current.pathlengths <- pathlengths(current.adj.matrix.cleaned.matrix.thr1,
                               weighted = TRUE)
  
  # compute small-worldness coefficient
  current.smallworldness <- smallworldness(current.adj.matrix.cleaned.matrix.thr1,
                                     iter = 1,
                                     progBar = FALSE,
                                     method = "HG")
  
  # summarize clustering, degree and path length by networks
  # to this end we first create a data frame with all the data and the network affiliation vector
  tmp.df = data.frame(clustering = current.clustering.weighted.thr1$CCi,
                      degree = current.degree.weighted.thr1,
                      pathlengths = current.pathlengths$ASPLi,
                      network = Community$names,
                      participation = current.participation$positive)
  
  # next we create the mean of each metric for each network in long format
  tmp.df.summary <- 
    tmp.df %>% 
    group_by(network) %>% 
    summarise(clustering = mean(clustering),
              degree = mean(degree),
              pathlength = mean(pathlengths),
              participation = mean(participation))
  
  # add subject ID
  tmp.df.summary$ID <- current.data$ID

  # concatenate network-specific data
  if(n == 1){tmp.df.summary.concat = tmp.df.summary}
  if(n > 1){tmp.df.summary.concat = rbind(tmp.df.summary.concat, tmp.df.summary)}

  # forward small-worldness, average shortest path length and community strength to the main data frame
  current.data$smallworldness <- current.smallworldness$swm
  current.data$ASPL <- current.pathlengths$ASPL
  current.data$degree.global = mean(current.degree.weighted.thr1)
  current.data$clustering.global = current.clustering.weighted.thr1$CC
  current.data$participation.global = mean(current.participation$overall)


  # adding in the community strength data is a bit more complicated, let me know if you want that explained in further detail
  current.comm.str.within.reshaped = data.frame(t(current.comm.str.within))
  names(current.comm.str.within.reshaped) <- paste0("comm.str.", names(current.comm.str.within.reshaped))
  current.data <- cbind(current.data, current.comm.str.within.reshaped)
  
  # concatenate main data frame
  if (n == 1){current.data.concat = current.data}
  if (n > 1){current.data.concat = rbind(current.data.concat,current.data)}

  
  }

# reshape network specific path length, clustering and degree
tmp.df.summary.concat2 <- data.frame(tmp.df.summary.concat)
tmp.df.wide = reshape(tmp.df.summary.concat2, idvar = "ID", timevar = "network", direction = "wide")

# merge all data together
output.data = merge(current.data.concat, tmp.df.wide, by = "ID")


```

```{r eval=FALSE, include=TRUE, warning=FALSE}
# define your input data
input.data = ADNI.data.bl

# define the number of iterations that need to be performed. 
# In our data frame, each row corresponds to one subject, so we want to iterate across rows.
# Thus, the number of iterations corresponds to the number of rows of our input data

n.iter = nrow(input.data)
print(paste0("number of iterations = ", n.iter))

# Next, open the loop, and define that it runs all 
# the way from 1 to the number of iterations that you've defined as n.iter
# within the loop, you will use the letter i which will change in each iteration from 1, to 2, to 3 to n.iter

n = 0
for (i in 1:n.iter){
  svMisc::progress(i, max.value = n.iter)
  # set an internal counter within the loop (can be helpful sometimes!)
  n = n +1
  
  # select the i-th observation in the data frame
  current.data = input.data[i,]
  
  # import the connectivity matrix
  current.adj.matrix = read.csv(current.data$FC.mat.200[1])
  
  # remove first column
  current.adj.matrix.cleaned <- current.adj.matrix[,-1]

  # clean diagnonal (i.e. remove infinite values with zeros)
  diag(current.adj.matrix.cleaned) = 0

  # transform to matrix format
  current.adj.matrix.cleaned.matrix <- 
    as.matrix(current.adj.matrix.cleaned)

  # remove negative connections and perform density 
  # thresholding at a density of 1
  current.adj.matrix.cleaned.matrix.thr1 <- 
    adj_mat_density_thresh(
      adj_matrix = current.adj.matrix.cleaned.matrix,
      retain_negatives = F, 
      density = 1, 
      replace_with_NAs = F)
  
  # compute clustering coefficient
  current.clustering.weighted.thr1 <-
    clustcoeff(current.adj.matrix.cleaned.matrix.thr1, 
               weighted = T)
  
  # compute degree
  current.degree.weighted.thr1 <-
    colMeans(current.adj.matrix.cleaned.matrix.thr1, 
             na.rm = T)

  # compute community strength
  current.network.connectivity.between = 
    comm.str(A = current.adj.matrix.cleaned.matrix.thr1,
         comm = Community$names,
         weighted = T,
         measure = c("between"))
  
  # compute participation coefficient
  current.network.connectivity.within = 
    participation(current.adj.matrix.cleaned.matrix.thr1, 
                  comm = Community$names)
  
  # compute path lengths
  current.pathlengths <- pathlengths(current.adj.matrix.cleaned.matrix.thr1,
                               weighted = TRUE)
  
  ## compute small-worldness coefficient
  #current.smallworldness <- smallworldness(current.adj.matrix.cleaned.matrix.thr1,
  #                                   iter = 1,
  #                                   progBar = FALSE,
  #                                   method = "HG")
  
  # summarize clustering, degree and path length by networks
  # to this end we first create a data frame with all the data and the network affiliation vector
  tmp.df = data.frame(clustering = 
                        current.clustering.weighted.thr1$CCi,
                      degree = 
                        current.degree.weighted.thr1,
                      network.connectivity.within =
                        current.network.connectivity.within$positive,
                      pathlengths = 
                        current.pathlengths$ASPLi,
                      network = 
                        Community$names)
  
  # next we create the mean of each metric for each network in long format
  tmp.df.summary <- 
    tmp.df %>% 
    group_by(network) %>% 
    summarise(clustering = mean(clustering),
              degree = mean(degree),
              pathlength = mean(pathlengths),
              network.connectivity.within = mean(network.connectivity.within))
  
  # add subject ID
  tmp.df.summary$ID <- current.data$ID

  # concatenate network-specific data
  if(n == 1){tmp.df.summary.concat = tmp.df.summary}
  if(n > 1){tmp.df.summary.concat = rbind(tmp.df.summary.concat, tmp.df.summary)}

  # forward small-worldness, average shortest path length and community strength to the main data frame
  #current.data$smallworldness <- current.smallworldness$swm
  current.data$ASPL <- current.pathlengths$ASPL
  current.data$degree.global = mean(current.degree.weighted.thr1)
  current.data$clustering.global = current.clustering.weighted.thr1$CC
  current.data$network.connectivity.within.global =
    mean(current.network.connectivity.within$overall)

  # adding in the community strength data is a bit more complicated, 
  # let me know if you want that explained in further detail
  current.network.connectivity.between.reshaped =
    data.frame(t(current.network.connectivity.between))
  
  names(current.network.connectivity.between.reshaped) <- 
    paste0("network.connectivity.between.", 
           names(current.network.connectivity.between.reshaped))
  
  current.data <- 
    cbind(current.data, current.network.connectivity.between.reshaped)
  
  # concatenate main data frame
  if (n == 1){current.data.concat = current.data}
  if (n > 1){current.data.concat = rbind(current.data.concat,current.data)}

  
  }

# reshape network specific path length, clustering and degree
tmp.df.summary.concat2 <- 
  data.frame(tmp.df.summary.concat)

tmp.df.wide = 
  reshape(tmp.df.summary.concat2, 
          idvar = "ID", 
          timevar = "network", 
          direction = "wide")

# merge all data together
output.data = merge(current.data.concat, tmp.df.wide, by = "ID")


```

### 4.2. Save your output

Since computations may take time, it's always a good idea to save your
output from time to time. Since our for-loop has generated quite some
data, it's time to save an updated spreadsheet, which we can load back
in afterwards and continue from there. We can easily write excel
spreadsheets using the *write_xlsx* function from *writexl* package. All
you need is a data frame *x* that you want to save as well as a full
*path* to an output file

```{r eval=FALSE, include=TRUE}
write_xlsx(x = output.data, 
           path = paste0(dir.root, "/data_sheets/ADNI_fMRI_PET_GraphTheory.xlsx"))
```

## 5. Analyze the data

first, import the data frame that we've saved before

```{r echo=TRUE}
ADNI.data.bl.graphtheory <- read_xlsx(paste0(dir.root, "/data_sheets/ADNI_fMRI_PET_GraphTheory.xlsx"))
ADNI.data.bl.graphtheory$DX.Ab <- factor(ADNI.data.bl.graphtheory$DX.Ab, 
                          levels = c("CN.Ab.neg", 
                                     "CN.Ab.pos", 
                                     "MCI.Ab.pos", 
                                     "Dementia.Ab.pos"))
```

### 5.1. What happens to the default mode network (DMN) in Alzheimer's disease patients?

Previous studies have suggested that the DMN is disrupted in Alzheimer's
disease patients. Attached, find a figure from a cohort with autosomal
dominantly inherited AD patients (i.e. a rare genetic form of the
disease, which becomes symptomatic around the age of 40). Here, you can
clearly see that the DMN becomes less pronounced in patients with
autosomal dominantly inherited AD (right) vs. controls (left). By the
way, if you want to learn more about the DMN, I recommend this excellent
review by Randy Buckner:
<https://www.nature.com/articles/s41583-019-0212-7>

```{r echo=FALSE, fig.align='center', out.width="60%"}
knitr::include_graphics(paste0(dir.root, "/images/DMN.jpg"))

```

Similar observations of DMN disruption have been made in patients with
sporadic AD. The DMN has been suggested to be crucial for episodic
memory performance, hence DMN disruptions have been argued to drive
memory decline.

```{r echo=FALSE, fig.align='center', out.width="60%"}
knitr::include_graphics(paste0(dir.root, "/images/DMN2.png"))

```

### 5.2. Does DMN connectivity change across the spectrum of AD

First, lets plot a simple measure of within DMN connectivity across
controls and AD patients. We have computed this measure in our for-loop
and saved it as *network.connectivity.within.DMN* in the
*ADNI.data.bl.graphtheory* data frame. We use ggplot again to create a
figure

```{r, fig.align='center'}

ggplot(data = ADNI.data.bl.graphtheory,
       aes( 
         x = DX.Ab,
         y = network.connectivity.within.DMN)) +
  geom_boxplot(notch = T, alpha = 0.1) + 
  geom_beeswarm() + 
  theme_minimal()

```

As you can see, there is a decrease in DMN connectivity across the AD
spectrum. But is this decrease significant? Lets use an ANCOVA to test
this.

```{r}
tmp.aov <- aov(data = ADNI.data.bl.graphtheory,
               network.connectivity.within.DMN ~ DX.Ab + age + sex); summary(tmp.aov)

```

Indeed, the data shows that the connectivity within the DMN decreases
across the AD spectrum.

```{r}
TukeyHSD(tmp.aov, which = "DX.Ab")

```

A post-hoc Tukey test shows that the patients with AD dementia mostly
drive this effect

Next, lets have a look whether the connectivity of the DMN to the
remainig brain also changes across the AD spectrum. We have also
computed this measure in our for-loop and saved it as
*network.connectivity.between.DMN* in the *ADNI.data.bl.graphtheory*
data frame. So same procedure as above....

```{r, fig.align='center'}

ggplot(data = ADNI.data.bl.graphtheory,
       aes( 
         x = DX.Ab,
         y = network.connectivity.between.DMN)) +
  geom_boxplot(notch = T, alpha = 0.1) + 
  geom_beeswarm() + 
  theme_minimal()

```

As you can see, there is a rather an increase in DMN connectivity to the
remaining brain across the AD spectrum. This suggests some sort of
de-differentiation of network activity (i.e. reduced within but
increased between network connectivity) But is this decrease
significant?

```{r}
tmp.aov <- aov(data = ADNI.data.bl.graphtheory,
               network.connectivity.between.DMN ~ DX.Ab + age + sex + PTEDUCAT); summary(tmp.aov)

```

Nope....So to sum up: What we find is a decrease in DMN connectivity
across the AD spectrum. But is this decrease in connectivity really
related to memory? Lets have a look at a scatterplot first

```{r, fig.align='center'}
ggplot(data = ADNI.data.bl.graphtheory,
       aes( 
         x = network.connectivity.within.DMN,
         y = ADNI_MEM)) +
  geom_point() + 
  geom_smooth(method = "lm") + 
  theme_minimal()
```

Next, lets check the correlation between DMN connectivity and memory
performance

```{r}
# compute correlation
rcorr(ADNI.data.bl.graphtheory$network.connectivity.within.DMN,
      ADNI.data.bl.graphtheory$ADNI_MEM)

```

Looks like there is a slight correlation between decreasing within DMN
connectivity and decreasing memory performance. But keep in mind that
"healthy controls" (i.e. CN.Ab.neg) are also included in this plot and
the correlation analysis. So in order to specifically investigate the
role of the DMN in AD patients, we need to create a subset of the data,
that is restricted to AD patients. To this end, we subset the data fram
*ADNI.data.bl.graphtheory* to subjects whose diagnosis is unequal to
(i.e. *!=*) *"CN.Ab.neg"*.

```{r}
# we store our subset in a dedicated data frame
ADNI.data.bl.graphtheory.AD.only <- subset(ADNI.data.bl.graphtheory, DX.Ab!="CN.Ab.neg")

```

Now we're left with AD patients only as you can see from the table
below.

```{r}
table(ADNI.data.bl.graphtheory.AD.only$DX.Ab)
```

So lets have a look again at the association between network
connectivity and memory performance, using our subset of AD patients
only:

```{r, fig.align='center'}
ggplot(data = ADNI.data.bl.graphtheory.AD.only,
       aes( 
         x = network.connectivity.within.DMN,
         y = ADNI_MEM)) +
  geom_point() + 
  geom_smooth(method = "lm") + 
  theme_minimal()

# compute correlation
rcorr(ADNI.data.bl.graphtheory.AD.only$network.connectivity.within.DMN,
      ADNI.data.bl.graphtheory.AD.only$ADNI_MEM)

```

Looks a bit stronger! So together, we found that DMN connectivity
decreases across the AD spectrum, and that a decrease in DMN
connectivity is associated with lower memory performance.


## TASK 3

***Next, you can try to run some additional analyses. For example, you can try to see connectivity of networks other than the DMN change throughout the disease course and whether they're related to cognitive function. Also, try to see how graph theoretical parameters are inter-related and how they relate to the AD groups.***
